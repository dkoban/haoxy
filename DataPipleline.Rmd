---
title: "Hoaxy Data Pipeline"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

The following document describes an automated workflow for the following tasks:

1. Collect Tweet data from Hoaxy
2. Enrich Twitter user accounts with profile information
3. Generate time series data for us in Vensim

## Load libraries and functions

```{r cars}
library(tidyverse)
library(stringr)
library(hoaxy)
library(RColorBrewer)
library(scales)
library(rtweet)
source("~/Documents/hoaxy/functions.R")
hoaxy_key('')
```

## Query Hoaxy for recent articles

```{r}
articles <- hx_latest_articles(past_hours = 30)
articles$tag <- extract_misinfo_tags(articles)
```

## Filter article list for likely misinformation stories

```{r}
misinfo_tags <- c("clickbait", "conspiracy", "junksci", 
                  "junkscience", "hoax", "fake")
articles <- articles %>% filter(tag %in% misinfo_tags)
articles$tag %>% table()
```

## Query articles for Hoaxy edges

Print sample headlines categorized as conspiracies.

```{r}
articles <- articles %>% filter(tag == "conspiracy")
articles$title[1:10]
```

Query Hoaxy for edges

```{r}
edges <- list()
for (i in 1:10){
edges[[i]] <- hx_edges(articles$id[i], nodes_limit = 50000)
print(paste0("Query ", i, " complete: ", nrow(edges[[i]]), " records"))
}
```

The story with the most tweet activity was:

```{r}
articles$title[edges %>% lapply(nrow) %>% unlist() %>% which.max()]
```

## Hoaxy edge data

List column names

```{r}
top_article <- edges[[edges %>% lapply(nrow) %>% unlist() %>% which.max()]]
top_article <- top_article %>% as_tibble()
top_article %>% colnames()
```

Count tweet activity by type

```{r}
top_article %>% count(tweet_type)
```

Plot tweet activty by date and time

```{r}
top_article <- top_article %>% 
  mutate(tweet_created_at = tweet_created_at %>% 
           as.POSIXct(format = "%Y-%m-%dT%H:%M:%S.000Z"))

ggplot(data = top_article,
         mapping = aes(x = tweet_created_at,
                       y = tweet_type)) +
    geom_jitter(size = 0.2) + 
    scale_x_datetime(breaks=date_breaks("24 hour"), labels=date_format("%m-%d")) +
    labs(title = top_article$title[1],
         x = "Tweet Created At",
         y = "Tweet Type")
```

## Parse time series data

Caculate differences between time of activity and time of first activity.

```{r}
top_article <- top_article %>% 
    mutate(h_hour = min(tweet_created_at)) %>%
    mutate(h = as.numeric(difftime(tweet_created_at, h_hour), units = "hours"))
```

Plot cumulative tweet activity and active users over time

- Activity: number of tweet `created_at` time stamps <= t
- Active User: count of unique users who have posted before time t

```{r}
time_increments <- top_article$h %>% unique() %>% sort()

activity_by_hour <- tibble(t = time_increments, 
                           tot_activity = NA,
                           tot_users = NA)

for (i in 1:length(time_increments)){
  activity_by_hour$tot_activity[i] = sum(top_article$h <= time_increments[i])
  activity_by_hour$tot_users[i] = 
    c(top_article$from_user_id[top_article$h <= time_increments[i]],
      top_article$to_user_id[top_article$h <= time_increments[i]]) %>%
    unique() %>% length()
  }

ggplot(data = activity_by_hour,
         mapping = aes(x = t, y = tot_activity)) +
  geom_point(size = 0.25, aes(color = "tot_activity")) + 
  geom_point(data = activity_by_hour,
             mapping = aes(x = t, y = tot_users,
                           color = "tot_users"),
             size = 0.25) +
  labs(title = "Cumulative Activity / Active Users by Hour",
       x = "Time (Hours)",
       y = "Count") 
```

Verify our cumulative user count matches the distinct user count from the original data

```{r}
# Distinct users in the original data
c(top_article$from_user_id, top_article$to_user_id) %>% unique() %>% length()
```

```{r}
# Max value of time series data
activity_by_hour$tot_users %>% max()
```


## Generate Vensim lookup variable text string

Round time values to integers and average counts for each period

```{r}
vensim_data <- activity_by_hour %>%
  mutate(t = t %>% ceiling()) %>%
  group_by(t) %>% 
  summarise(tot_users = tot_users %>% mean() %>% ceiling())

N <- nrow(vensim_data)
vensim_data$change <- c(0,
      vensim_data$tot_users[2:N] - vensim_data$tot_users[1:(N-1)])
vensim_data %>% head()
```

Generate lookup table string where each value represents 

```{r}
vensim_string <- c()
for (i in 1:nrow(vensim_data)){
  vensim_string[i] <- paste0("(", vensim_data$t[i], ", ", 
                             vensim_data$change[i], ")")}

vensim_string <- c(vensim_string[-length(vensim_string)])
vensim_string <- vensim_string %>% paste0(collapse = ", ")
vensim_string
```

## Enrich with Twitter account information

Enter API keys.

```{r}
twitter_token <- create_token(
  app = "",
  consumer_key = "",
  consumer_secret = "",
  access_token = "",
  access_secret = "")
```

Query Twitter for user info.

```{r}
unique_users <- c(top_article$from_user_id, top_article$to_user_id) %>% 
  unique()
user_info <- lookup_users(users = unique_users)
follower_counts <- user_info %>% select(user_id, screen_name, followers_count)
follower_counts %>% arrange(desc(followers_count)) %>% head(10)
```

Merge with the edge data

```{r}
merged <- top_article %>% 
  left_join(follower_counts %>% 
              select(from_user_id = user_id, 
                     from_followers_count = followers_count)) %>%
  left_join(follower_counts %>% 
              select(to_user_id = user_id, 
                     to_followers_count = followers_count))

merged %>% select(from_user_screen_name, from_followers_count,
                  to_user_screen_name, to_followers_count,
                  tweet_type, tweet_created_at)
```

## Identify burst seeds 

```{r}
burst_seed_window <- c(0, 0.15)
burst1_seeds <- merged %>% filter(h >= burst_seed_window[1],
                                  h <= burst_seed_window[2]) %>%
  select(from_user_id, from_user_screen_name, from_followers_count) %>% 
  distinct()
burst1_seeds
```

```{r}
burst_seed_window <- c(14, 15)
burst2_seeds <- merged %>% filter(h >= burst_seed_window[1],
                                  h <= burst_seed_window[2]) %>%
  select(from_user_id, from_user_screen_name, from_followers_count) %>% 
  distinct()
burst2_seeds
```

## Average burst seed follower counts

```{r}
burst1_followers <- burst1_seeds$from_followers_count %>% mean()
burst2_followers <- burst2_seeds$from_followers_count %>% mean()

print(paste0("Burst-1: ", nrow(burst1_seeds), " seeds, ",
             ceiling(burst1_followers), " followers"))
print(paste0("Burst-2: ", nrow(burst2_seeds), " seeds, ",
       ceiling(burst2_followers), " followers"))
```